[00:00:00]
>> Scott Moss: I'm gonna move on to caching. And when I talk about caching in GraphQL, when I say the word caching in a server, what do people think?
>> Student: Redis.
>> Scott Moss: You think Redis? Okay, what about you?
>> Student2: Well, I'm thinking about exactly what you're talking about, so-
>> Scott Moss: Okay, not you [LAUGH] What about you?

[00:00:22]

>> Student3: I don't even have an idea.
>> Scott Moss: All right, sure.
>> Student4: It's the request on the request level.
>> Scott Moss: We're caching the request.
>> Student4: And then caching in memory units of data. So like with Redis or something in memory.
>> Scott Moss: Yeah, yeah, all those are caching on the back end.

[00:00:45]
And there's tons more, right? People are caching database crazed, not even in Redis, so it's like on the database. So it's like, my god. So there's so many different ways to cache. What I'm going to specifically be talking about, at least for this example, is going to be caching in memory per request.

[00:01:03]
And the way we do that is something called a data loader in GraphQL. So this one's kinda complicated, but it is so worth it. So what I'm gonna do here is, am I on the right branch?
>> Scott Moss: Let me check out if it's the right branch.
>> Scott Moss: All right.

[00:01:31]
So the problem we're having is, let's talk about the problem first. Is basically what I've been discussing this whole time. So let's say we have, we're querying for projects. And then we're querying for tasks. And then inside that task query, we're also querying for projects and then we're also querying for task.

[00:01:50]
Okay, and let's just get the name. Now if we just look at this, I know for each one of these projects, they're going to have their own individual task. Right, and then each one of these tasks are going to have the same project that resolved it in the first place.

[00:02:07]
So that means we already created the database again for something that we already previously did up here. So that's inefficient. And then two, we did it again when we asked for the same project that we already queried for. What your tasks are, we already did that up here, so that's also very inefficient.

[00:02:22]
So this is becoming exponential every time we go down, right? So yes, there are ways to get around that, you can short circuit it, but you can also make this faster by caching the results or batching and caching the results of what you already resolved. And that's where data load comes in, it's going to batch the calls to your database.

[00:02:46]
And then cache them so when you ask for them in the same request on the same level, they'll just get them back to you instantly because its cached in memory. And then that memory is not long lived. It's only per request, so therefore you're not sharing state across multiple requests for different people.

[00:03:03]
It would be dangerous if my request came in and it cached my data and then you sent a request with the same GraphQL query and you got my cached data. Right, that's dangerous. So it's only per request, and that's what a data loader is for. You could think it was a per request, in memory batching, caching layer for your database.

[00:03:23]
Sound ridiculous, I know. But let's walk through.

